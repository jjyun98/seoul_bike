import requests
import pandas as pd
import numpy as np
import time
from datetime import datetime
import boto3
from io import BytesIO

# --- Core Functions ---

def get_rent_history_by_day(api_key, year, month, day):
    """ì§€ì •ëœ ë‚ ì§œì˜ ëª¨ë“  ëŒ€ì—¬ ì´ë ¥ì„ ì‹œê°„ëŒ€ë³„/ì²­í¬ë³„ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    all_data = []
    date_str = f"{year}-{month:02d}-{day:02d}"
    
    print(f"[{date_str}] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    
    for hour in range(24):
        try:
            # 1. í•´ë‹¹ ì‹œê°„ëŒ€ì˜ ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸
            url = f"http://openapi.seoul.go.kr:8088/{api_key}/json/tbCycleRentData/1/5/{date_str}/{hour}"
            response = requests.get(url, timeout=20)
            response.raise_for_status()
            
            data = response.json()
            
            if 'rentData' not in data or 'list_total_count' not in data['rentData']:
                continue
            
            total_count = int(data['rentData']['list_total_count'])
            if total_count == 0:
                continue

            # 2. 1000ê°œ ë‹¨ìœ„ë¡œ í˜ì´ì§•í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘
            for start in range(1, total_count + 1, 1000):
                end = min(start + 999, total_count)
                page_url = f"http://openapi.seoul.go.kr:8088/{api_key}/json/tbCycleRentData/{start}/{end}/{date_str}/{hour}"
                
                page_response = requests.get(page_url, timeout=20)
                page_data = page_response.json()
                
                if 'rentData' in page_data and 'row' in page_data['rentData']:
                    rows = page_data['rentData']['row']
                    all_data.extend(rows)
                
                time.sleep(0.1) # API ë¶€í•˜ ê°ì†Œ
                
        except requests.exceptions.RequestException as e:
            print(f"!! [{date_str} {hour:02d}ì‹œ] API ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue
        except Exception as e:
            print(f"!! [{date_str} {hour:02d}ì‹œ] ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
            
    print(f"[{date_str}] ì´ {len(all_data)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return pd.DataFrame(all_data)


def load_valid_stations(bucket_name):
    """S3ì—ì„œ ìœ íš¨í•œ ì •ê±°ì¥ ID ëª©ë¡ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        s3 = boto3.client('s3')
        key = 'station/station.csv'
        obj = s3.get_object(Bucket=bucket_name, Key=key)
        df_station = pd.read_csv(BytesIO(obj['Body'].read()))
        
        valid_stations = set(df_station['station_id'])
        print(f"âœ… ìœ íš¨ ì •ê±°ì¥ {len(valid_stations)}ê°œ ë¡œë“œ ì™„ë£Œ (from s3://{bucket_name}/{key})")
        return valid_stations
    except Exception as e:
        print(f"ğŸ›‘ S3ì—ì„œ ì •ê±°ì¥ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def upload_parquet_to_s3(df, bucket_name, file_path):
    """DataFrameì„ Parquet í˜•ì‹ìœ¼ë¡œ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    if df.empty:
        print(f"âš ï¸ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ì–´ s3://{bucket_name}/{file_path} ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    parquet_buffer = BytesIO()
    df.to_parquet(parquet_buffer, index=False, engine='pyarrow')
    parquet_buffer.seek(0)
    
    s3 = boto3.client('s3')
    s3.put_object(
        Bucket=bucket_name,
        Key=file_path,
        Body=parquet_buffer.getvalue()
    )
    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: s3://{bucket_name}/{file_path} ({len(df)}ê±´)")


# --- Main Orchestration Function ---

def collect_and_upload_monthly_data(api_key, bucket, year, month):
    """
    ì›”ë³„ ëŒ€ì—¬ ì´ë ¥ì„ ìˆ˜ì§‘, ì „ì²˜ë¦¬í•˜ì—¬ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ì¼ë³„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ë§ˆì§€ë§‰ì— ë³‘í•©í•©ë‹ˆë‹¤.
    """
    print(f"ğŸš€ ===== {year}ë…„ {month}ì›” ëŒ€ì—¬ ì´ë ¥ ì²˜ë¦¬ ì‹œì‘ ====")
    
    # 1. ìœ íš¨ ì •ê±°ì¥ ì •ë³´ ë¡œë“œ (ì‘ì—… ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
    valid_stations = load_valid_stations(bucket)
    
    # 2. í•´ë‹¹ ì›”ì˜ ëª¨ë“  ë‚ ì§œì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
    processed_daily_dfs = []
    days_in_month = pd.Period(f'{year}-{month}').days_in_month
    
    for day in range(1, days_in_month + 1):
        try:
            # ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘
            df_day = get_rent_history_by_day(api_key, year, month, day)
            if df_day.empty:
                continue
            
            # ì¼ë³„ ë°ì´í„° ì „ì²˜ë¦¬ (PK ìƒì„± ì œì™¸)
            df_processed = _preprocess_rent_df(df_day, valid_stations)
            if not df_processed.empty:
                processed_daily_dfs.append(df_processed)
                
        except Exception as e:
            print(f"!! {year}-{month}-{day} ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
            
    if not processed_daily_dfs:
        print(f"ğŸ›‘ {year}ë…„ {month}ì›”ì— ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 3. ëª¨ë“  ì¼ë³„ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©
    print("\nğŸ”„ ëª¨ë“  ì¼ë³„ ë°ì´í„° ë³‘í•© ë° ìµœì¢… ì²˜ë¦¬ ì¤‘...")
    df_month = pd.concat(processed_daily_dfs, ignore_index=True)
    print(f"ì›” ì´ ë°ì´í„°: {len(df_month)}ê±´")
    
    # 4. PK ìƒì„± ë° ìµœì¢… ì •ë¦¬
    df_final = _finalize_rent_df(df_month)

    # 5. S3ì— Parquetìœ¼ë¡œ ì—…ë¡œë“œ
    file_path = f"rent/{year}/month_{month:02d}.parquet"
    upload_parquet_to_s3(df_final, bucket, file_path)
    
    print(f"ğŸ‰ ===== {year}ë…„ {month}ì›” ëŒ€ì—¬ ì´ë ¥ ì²˜ë¦¬ ì™„ë£Œ ====")


# --- Helper Functions for Preprocessing ---

def _preprocess_rent_df(df, valid_stations):
    """ëŒ€ì—¬ ì´ë ¥ ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ ì „ì²˜ë¦¬ (PK ìƒì„± ì œì™¸)"""
    
    # 1. ìŠ¤í‚¤ë§ˆ í™•ì¸ ë° ì»¬ëŸ¼ëª… í‘œì¤€í™”
    if 'ëŒ€ì—¬ì¼ì‹œ' in df.columns: # êµ¬ ìŠ¤í‚¤ë§ˆ (í•œê¸€ ì»¬ëŸ¼ëª…)
        df = df.rename(columns={
            'ëŒ€ì—¬ì¼ì‹œ': 'rent_datetime', 'ë°˜ë‚©ì¼ì‹œ': 'return_datetime',
            'ì´ìš©ê±°ë¦¬(M)': 'distance', 'ìƒë…„': 'birth_year', 'ì„±ë³„': 'gender',
            'ì´ìš©ìì¢…ë¥˜': 'user_type', 'ëŒ€ì—¬ëŒ€ì—¬ì†ŒID': 'rent_station_id',
            'ë°˜ë‚©ëŒ€ì—¬ì†ŒID': 'return_station_id', 'ìì „ê±°êµ¬ë¶„': 'bike_type'
        })
    elif 'RENT_DT' in df.columns: # ì‹  ìŠ¤í‚¤ë§ˆ (ì˜ë¬¸ ì»¬ëŸ¼ëª…)
        df = df.rename(columns={
            'RENT_DT': 'rent_datetime', 'RTN_DT': 'return_datetime',
            'USE_DST': 'distance', 'BIRTH_YEAR': 'birth_year',
            'SEX_CD': 'gender', 'USR_CLS_CD': 'user_type',
            'RENT_STATION_ID': 'rent_station_id', 'RETURN_STATION_ID': 'return_station_id',
            'BIKE_SE_CD': 'bike_type'
        })
    else:
        print(f"âš ï¸ ì»¬ëŸ¼ ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜. ê±´ë„ˆëœë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}")
        return pd.DataFrame()

    # 2. í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (í‘œì¤€í™”ëœ ì´ë¦„ìœ¼ë¡œ)
    required_cols = ['rent_datetime', 'return_datetime', 'rent_station_id', 'return_station_id']
    if not all(col in df.columns for col in required_cols):
        print(f"âš ï¸ í‘œì¤€í™” í›„ í•„ìˆ˜ ì»¬ëŸ¼ ë¶€ì¡±. ê±´ë„ˆëœë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {df.columns.tolist()}")
        return pd.DataFrame()

    # 3. ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±° (ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ì›ë³¸ ì»¬ëŸ¼ë“¤)
    cols_to_drop = [
        'ëŒ€ì—¬ ëŒ€ì—¬ì†Œëª…', 'ë°˜ë‚©ëŒ€ì—¬ì†Œëª…', 'ìì „ê±°ë²ˆí˜¸', 'ëŒ€ì—¬ê±°ì¹˜ëŒ€', 'ë°˜ë‚©ê±°ì¹˜ëŒ€', 
        'ì´ìš©ì‹œê°„(ë¶„)', 'ë°˜ë‚©ëŒ€ì—¬ì†Œë²ˆí˜¸', 'ëŒ€ì—¬ ëŒ€ì—¬ì†Œë²ˆí˜¸', 'RENT_ID', 'RENT_NM', 
        'RENT_HOLD', 'RTN_ID', 'RTN_NM', 'RTN_HOLD', 'USE_MIN', 'BIKE_ID',
        'START_INDEX', 'END_INDEX', 'RNUM'
    ]
    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')

    # 4. '\N' -> NaN
    df = df.replace('\\N', np.nan)

    # 5. íƒ€ì… ë³€í™˜ (í‘œì¤€í™”ëœ ì´ë¦„ìœ¼ë¡œ)
    df['rent_datetime'] = pd.to_datetime(df['rent_datetime'], errors='coerce')
    df['return_datetime'] = pd.to_datetime(df['return_datetime'], errors='coerce')
    if 'birth_year' in df.columns:
        df['birth_year'] = pd.to_numeric(df['birth_year'], errors='coerce')
    if 'distance' in df.columns:
        df['distance'] = pd.to_numeric(df['distance'], errors='coerce')
    
    # 6. í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ë°ì´í„° ì œê±°
    df.dropna(subset=['rent_datetime', 'return_datetime', 'rent_station_id', 'return_station_id'], inplace=True)

    # 7. ì™¸ë˜í‚¤ ë¬´ê²°ì„± ì²´í¬
    df = df[
        df['rent_station_id'].isin(valid_stations) &
        df['return_station_id'].isin(valid_stations)
    ]

    # 8. ì´ìƒì¹˜ ì œê±°
    if 'distance' in df.columns:
        df['distance'] = pd.to_numeric(df['distance'], errors='coerce').fillna(0)
        df = df[df['distance'] >= 0]
    
    if 'birth_year' in df.columns:
        df = df[
            (df['birth_year'].isna()) |
            ((df['birth_year'] >= 1920) & (df['birth_year'] <= datetime.now().year))
        ]
        
    print(f"ê¸°ë³¸ ì „ì²˜ë¦¬ í›„ {len(df)}ê±´")
    return df

def _finalize_rent_df(df):
    """ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ì— PKë¥¼ ìƒì„±í•˜ê³  ì»¬ëŸ¼ ìˆœì„œë¥¼ ì •ë ¬"""
    if df.empty:
        return df

    df = df.sort_values(by='rent_datetime').reset_index(drop=True)
    
    # PK ì¶”ê°€: 'YYYYMMDDHH' + ì‹œê°„ë³„ ìˆœë²ˆ (6ìë¦¬)
    df['rental_id'] = (
        df['rent_datetime'].dt.strftime('%Y%m%d%H') +
        df.groupby(df['rent_datetime'].dt.strftime('%Y%m%d%H')).cumcount().add(1).astype(str).str.zfill(6)
    )

    # ë§¨ ì•ìœ¼ë¡œ ì´ë™
    cols = ['rental_id'] + [col for col in df.columns if col != 'rental_id']
    df = df[cols]
    
    print(f"ìµœì¢… ì²˜ë¦¬ í›„ {len(df)}ê±´ (PK ìƒì„± ì™„ë£Œ)")
    return df

# --- [Optional] Test function for single day ---
def collect_and_upload_day_for_test(api_key, bucket, year, month, day):
    """(í…ŒìŠ¤íŠ¸ìš©) í•˜ë£¨ì¹˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì „ì²˜ë¦¬í•˜ì—¬ S3ì— ì—…ë¡œë“œ"""
    print(f"ğŸš€ ===== {year}-{month}-{day} í…ŒìŠ¤íŠ¸ ì‹œì‘ ====")
    valid_stations = load_valid_stations(bucket)
    df_day = get_rent_history_by_day(api_key, year, month, day)
    if not df_day.empty:
        df_processed = _preprocess_rent_df(df_day, valid_stations)
        df_final = _finalize_rent_df(df_processed)
        file_path = f"rent/test/{year}{month:02d}{day:02d}.parquet"
        upload_parquet_to_s3(df_final, bucket, file_path)
    print(f"ğŸ‰ ===== í…ŒìŠ¤íŠ¸ ì¢…ë£Œ =====")


def process_entire_year(api_key, bucket, year):
    """ì§€ì •ëœ ì—°ë„ì˜ 1ì›”ë¶€í„° 12ì›”ê¹Œì§€ ëª¨ë“  ì›”ë³„ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print(f"ğŸš€ğŸš€ğŸš€ ===== {year}ë…„ ì „ì²´ ëŒ€ì—¬ ì´ë ¥ ì²˜ë¦¬ ì‹œì‘ ===== ğŸš€ğŸš€ğŸš€")
    for month in range(1, 13):
        try:
            collect_and_upload_monthly_data(api_key, bucket, year, month)
        except Exception as e:
            print(f"ğŸ›‘ğŸ›‘ğŸ›‘ {year}ë…„ {month}ì›” ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ {year}ë…„ ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤: {e}")
            # í•œ í•´ì˜ íŠ¹ì • ì›”ì—ì„œ ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ ì—°ë„ ì „ì²´ë¥¼ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ê³  ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
            raise
    print(f"ğŸ‰ğŸ‰ğŸ‰ ===== {year}ë…„ ì „ì²´ ëŒ€ì—¬ ì´ë ¥ ì²˜ë¦¬ ì™„ë£Œ ===== ğŸ‰ğŸ‰ğŸ‰")
